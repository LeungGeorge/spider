2017-11-05 22:32:41 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: collectips)
2017-11-05 22:32:41 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'collectips.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['collectips.spiders'], 'BOT_NAME': 'collectips', 'LOG_FILE': 'logs/collectips/xicidaili/2da251f3c23611e7a38ea45e60dcadc5.log', 'DOWNLOAD_DELAY': 1}
2017-11-05 22:32:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-11-05 22:32:41 [twisted] CRITICAL: Unhandled error in Deferred:
2017-11-05 22:32:41 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/twisted/internet/defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python2.7/site-packages/scrapy/crawler.py", line 95, in crawl
    six.reraise(*exc_info)
  File "/usr/local/lib/python2.7/site-packages/scrapy/crawler.py", line 76, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/scrapy/crawler.py", line 99, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/scrapy/spiders/__init__.py", line 51, in from_crawler
    spider = cls(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument '_job'
